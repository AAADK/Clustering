/*
 * Ribosomal Database Project II
 * Copyright 2009 Michigan State University Board of Trustees
 */
package edu.msu.cme.rdp.hadoop.utils;

import edu.msu.cme.pyro.cluster.Clustering;
import edu.msu.cme.pyro.cluster.dist.ThinEdge;
import edu.msu.cme.pyro.cluster.utils.ClusterFactory;
import edu.msu.cme.pyro.cluster.io.ClusterFileOutput;
import edu.msu.cme.pyro.cluster.io.ClusterOutput;
import edu.msu.cme.pyro.cluster.io.EdgeReader;
import edu.msu.cme.pyro.cluster.io.EdgeWriter;
import edu.msu.cme.pyro.derep.IdMapping;
import edu.msu.cme.pyro.derep.SampleMapping;
import java.io.File;
import java.io.IOException;
import java.io.PrintStream;
import java.util.ArrayList;
import java.util.List;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.util.GenericOptionsParser;
import edu.msu.cme.rdp.hadoop.distance.mapred.keys.*;
import org.apache.hadoop.mapred.*;

/**
 *
 * @author farrisry
 */
public class HadoopClusteringOneOff {

    private static void printUsageAndExit() {
        System.err.println("usage: <merges_out> <jobs_dir> partitionFile0 partitionFile1 ...");
        System.exit(1);
    }

    /**
     * @param args the command line arguments
     */
    public static void main(String... args) throws IOException {
        Configuration conf = new Configuration();
        args = new GenericOptionsParser(conf, args).getRemainingArgs();

        if (args.length == 0) {
            printUsageAndExit();
        }

        ClusterOutput clusterOut = null;
        ClusterFactory f = null;
        float step = -1;
        int fIndex = 3;
	f = new ClusterFactory(new File(args[0]));
	fIndex = 2;
	clusterOut = new ClusterOutput() {
                public void printClusters(ClusterFactory factory, int step) {
                }
		
                public void close() {
                }
	    };

	
        List<Path> files = new ArrayList<Path>();

        for (int i = 2; i < args.length; i++) {
            files.add(new Path(args[i]));
        }
	final HDFSEdgeReader er = new HDFSEdgeReader(conf, files);
	final CombinedIntermediateSortedReader ir = CombinedIntermediateSortedReader.fromDirectory(conf, new File(args[1]));

        EdgeReader reader = new CombinedReader(er, ir);

        Clustering.doCompleteLinkage(f, reader, step, clusterOut);
        
        if(clusterOut != null) {
            clusterOut.close();
        }

	f.finish();
    }

    private static class CombinedReader implements EdgeReader {
	private DistanceAndComparison hdfsNext;
	private DistanceAndComparison irNext;
	private HDFSEdgeReader er;
	private CombinedIntermediateSortedReader ir;

	public CombinedReader(HDFSEdgeReader er, CombinedIntermediateSortedReader ir) throws IOException {
	    this.er = er;
	    this.ir = ir;

	    nextEr();
	    nextIr();
	}

	public void close() throws IOException {
	    er.close();
	    ir.close();
	}

	private void nextEr() throws IOException {
	    ThinEdge tmp = er.nextThinEdge();
	    if(tmp == null) {
		hdfsNext = null;
		return;
	    }

	    hdfsNext = new DistanceAndComparison();
	    hdfsNext.first = tmp.getSeqi();
	    hdfsNext.second = tmp.getSeqj();
	    hdfsNext.distance = tmp.getDist();
	}

	private void nextIr() throws IOException {
	    irNext = ir.next();
	}

	public ThinEdge nextThinEdge() throws IOException {
	    DistanceAndComparison ret = null;
	    if(hdfsNext == null && irNext == null) {
		return null;
	    } else if(hdfsNext == null) {
		ret = irNext;
	    } else if(irNext == null) {
		ret = hdfsNext;
	    } else {
		if(irNext.compareTo(hdfsNext) < 0) {
		    ret = irNext;
		    nextIr();
		} else {
		    ret = hdfsNext;
		    nextEr();
		}
	    }

	    return new ThinEdge(ret.first, ret.second, ret.distance);
	}
    }

    public static void dumpDists(String... args) throws IOException {

        Configuration conf = new Configuration();
        args = new GenericOptionsParser(conf, args).getRemainingArgs();


        if (args.length < 1) {
            System.err.println("usage: hdfs-partion-file ...");
            System.exit(1);
        }

        List<Path> files = new ArrayList<Path>();

        for (int i = 0; i < args.length; i++) {
            files.add(new Path(args[i]));
        }

        EdgeReader reader = new HDFSEdgeReader(conf, files);
        ThinEdge edge;
        int lastEdge = Integer.MIN_VALUE;

        while ((edge = reader.nextThinEdge()) != null) {
            if (edge.getDist() < lastEdge) {
                System.out.println(" LAST > NEW: " + lastEdge + " : " + edge.getSeqi() + " " + edge.getSeqj() + " " + edge.getDist());
            }
            System.out.println(edge.getSeqi() + " " + edge.getSeqj() + " " + edge.getDist());
            lastEdge = edge.getDist();
        }
    }

    public static void dumpBinDists(String... args) throws IOException {

        Configuration conf = new Configuration();
        args = new GenericOptionsParser(conf, args).getRemainingArgs();


        if (args.length < 2) {
            System.err.println("usage: outfile hdfs-partion-file ...");
            System.exit(1);
        }

        List<Path> files = new ArrayList<Path>();

        for (int i = 1; i < args.length; i++) {
            files.add(new Path(args[i]));
        }

        EdgeReader reader = new HDFSEdgeReader(conf, files);
        ThinEdge edge;
        int lastEdge = Integer.MIN_VALUE;
        EdgeWriter writer = new EdgeWriter(new File(args[0]));

        long readEdges = 0;
        try {
            while ((edge = reader.nextThinEdge()) != null) {
                if (edge.getDist() < lastEdge) {
                    System.err.println(" LAST > NEW: " + lastEdge + " : " + edge.getSeqi() + " " + edge.getSeqj() + " " + edge.getDist());
                }

                writer.writeEdge(edge);
                lastEdge = edge.getDist();

                readEdges++;
                if (readEdges % 10000000 == 0) {
                    System.out.printf("Dumped %n edges", readEdges);
                }
            }
            writer.close();
        } catch (Exception e) {
            writer.close();
            new File(args[0]).delete();
        }
    }
}
